
```markdown
# Sales Data ETL Pipeline

A PySpark-based ETL pipeline that processes daily sales data from AWS S3, enriches it with dimension tables, and generates business intelligence reports.

---

## ğŸ“Œ Project Overview

This project automates the processing of sales transactions to answer two key business questions:
1. **How much does each customer spend every month?**
2. **Which salesperson is the top performer and what's their incentive?**

**Learning Project**: Built following [Manish Kumar's](https://www.youtube.com/@TrendyTech) Data Engineering tutorial with significant modifications and enhancements.

---

## ğŸ¯ Business Goals

### 1. Customer Monthly Purchase Analysis
Calculate total spending per customer per month for:
- Customer retention analysis
- Spending pattern identification
- Targeted marketing campaigns

### 2. Sales Team Incentive Calculation
Identify top sales performer each month and calculate:
- Total sales per salesperson
- Top performer gets **1% incentive** of their sales
- Others get no incentive

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        AWS S3 (Source)                      â”‚
â”‚                    to_process/sales_*.csv                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Data Validation Layer                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ File Type    â”‚  â”‚   Schema     â”‚  â”‚  Extra       â”‚       â”‚
â”‚  â”‚ Validation   â”‚â†’ â”‚  Validation  â”‚â†’ â”‚  Columns     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                PySpark Transformation Layer                 â”‚
â”‚                                                             â”‚
â”‚   Sales Data  +  Customer Table  +  Store Table             â”‚
â”‚                  +  Sales Team Table                        â”‚
â”‚                         â†“                                   â”‚
â”‚                  Enriched Dataset                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Output Layer                           â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  AWS S3        â”‚  â”‚   AWS S3       â”‚  â”‚   MySQL      â”‚   â”‚
â”‚  â”‚  Customer Mart â”‚  â”‚   Sales Mart   â”‚  â”‚   Metrics    â”‚   â”‚
â”‚  â”‚  (Parquet)     â”‚  â”‚   (Partitioned â”‚  â”‚   Tables     â”‚   â”‚
â”‚  â”‚                â”‚  â”‚    Parquet)    â”‚  â”‚              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Data Flow

```
1. Extract
   S3 (to_process/) â†’ Download to Local
   
2. Validate
   â”œâ”€ CSV files only â†’ Continue
   â”œâ”€ Non-CSV â†’ error_files/wrong_file_types/
   â”œâ”€ Valid schema â†’ Continue
   â””â”€ Invalid schema â†’ error_files/bad_schema/

3. Transform
   Sales Data + Dimension Tables (Customer, Store, Sales Team)
   â†’ Enriched Dataset with full details

4. Load
   â”œâ”€ Customer Data Mart â†’ S3 (Parquet)
   â”œâ”€ Sales Data Mart â†’ S3 (Partitioned Parquet by month & store)
   â”œâ”€ Customer Monthly Purchase â†’ MySQL
   â””â”€ Sales Team Incentive â†’ MySQL

5. Archive
   Processed files â†’ processed/{timestamp}/
   Update staging table â†’ status = 'COMPLETED'
```

---

## ğŸ› ï¸ Technologies

- **Processing**: Apache Spark 3.5, PySpark
- **Storage**: AWS S3, MySQL 8.0
- **Language**: Python 3.10
- **Cloud**: AWS (S3, boto3)
- **Format**: Parquet, CSV

---

## ğŸ“ Project Structure

```
sales-etl-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                        # Main pipeline orchestration
â”‚   â”œâ”€â”€ extract/extract.py             # S3 file extraction
â”‚   â”œâ”€â”€ transform/transform.py         # Data validation & transformation
â”‚   â”œâ”€â”€ load/write.py                  # Write to S3 & local
â”‚   â”œâ”€â”€ move/move.py                   # File movement operations
â”‚   â”œâ”€â”€ staging/staging.py             # Staging table management
â”‚   â”œâ”€â”€ utils/utility.py               # Spark, S3, DB utilities
â”‚   â””â”€â”€ utils/logging_config.py        # Logging config
â”œâ”€â”€ config.py                          # Configuration variables
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ’¡ What I Learned

### Technical Skills
âœ… **PySpark** - Large-scale data processing with DataFrames and SQL  
âœ… **Dimensional Modeling** - Star schema design for analytics  
âœ… **AWS S3 Integration** - Reading/writing data to cloud storage  
âœ… **ETL Design Patterns** - Extract, Transform, Load workflows  
âœ… **Data Quality** - Schema validation, error handling, idempotency  
âœ… **SQL** - Complex joins, aggregations, window functions  
âœ… **Fault-tolerant design** with idempotency and crash recovery

### Best Practices
âœ… **Error Handling** - Segregated error folders, retry logic  
âœ… **Logging** - Comprehensive pipeline tracking  
âœ… **Idempotency** - Safe to re-run without duplicates  
âœ… **Code Organization** - Modular, function-based structure  
âœ… **Documentation** - Clear comments and docstrings  

---

## ğŸ¨ Key Modifications from Original Tutorial

| Feature            | Original Tutorial        | My Implementation                                          |
|--------------------|--------------------------|------------------------------------------------------------|
| **Structure**      | Single file (~500 lines) | Modular: 7 separate files (extract, transform, load, etc.) |
| **Error Handling** | One `error/` folder      | Segregated: `wrong_file_types/`, `bad_schema/`, `unknown/` |
| **S3 Writes**      | Write local â†’ upload     | **Direct S3 write** (reduced I/O)                          |
| **Partitioning**   | Hardcoded columns        | **Generic function** with `partitionby` parameter          |
| **File Archiving** | Overwrites files         | **Timestamp folders** (YYYYMMDD_HHMMSS)                    |
| **Functions**      | Inline code              | **15+ reusable functions** with single responsibility      |

### ğŸ’¡ Enhancements I did

#### 1. Direct S3 Write
```python
# Before: 2 steps (write local â†’ upload)
df.write.parquet("./local/file.parquet")
s3_client.upload_file(local_path, bucket, key)

# After: 1 step (direct write)
df.write.parquet(f"s3a://{bucket}/{key}")
```

#### 2. Flexible Partitioning
```python
# Can handle both partitioned and non-partitioned writes
write_parquet_to_s3(df, bucket, key)  # No partition
write_parquet_to_s3(df, bucket, key, partitionby=["month", "store"])  # With partition
```

#### 3. Organized Error Management
```
Before: data/error_files/ (everything mixed)

After:
data/error_files/
â”œâ”€â”€ wrong_file_types/     # .txt, .xlsx, etc.
â”œâ”€â”€ bad_schema/           # Missing mandatory columns
â””â”€â”€ unknown/              # Leftover from previous failed runs
```

#### 4. Timestamp-Based Archiving
```
Before: processed/sales_data.csv (overwrites each run)

After:
processed/
â”œâ”€â”€ 20240215_143022/sales_data.csv
â””â”€â”€ 20240216_091533/sales_data.csv
```

---

## ğŸ“Š Output Data

### Customer Data Mart (S3 Parquet)
```
customer_id | first_name | last_name | sales_date | total_cost
```

### Sales Team Data Mart (S3 Partitioned Parquet)
```
Partitioned by: sales_month, store_id

Fields: sales_person_id, first_name, last_name, 
        sales_date, total_cost, store_id
```

### Customer Monthly Purchase (MySQL)
```sql
+-------------+------------+-----------+------+-------+--------------+
| customer_id | first_name | last_name | year | month | monthly_total|
+-------------+------------+-----------+------+-------+--------------+
| 1           | John       | Doe       | 2024 | 1     | 5000.00      |
| 1           | John       | Doe       | 2024 | 2     | 7500.00      |
+-------------+------------+-----------+------+-------+--------------+
```

### Sales Team Incentive (MySQL)
```sql
+----------------+------------+-----------+------+-------+-------------+-----------+
| sales_person_id| first_name | last_name | year | month | total_sales | incentive |
+----------------+------------+-----------+------+-------+-------------+-----------+
| 101            | Alice      | Johnson   | 2024 | 1     | 50000.00    | 500.00    |
| 102            | Bob        | Williams  | 2024 | 1     | 45000.00    | 0.00      |
+----------------+------------+-----------+------+-------+-------------+-----------+
```
*Top performer (Alice) gets 1% incentive*

---


## ğŸ”® Future Enhancements

- [ ] Apache Airflow for scheduling
- [ ] Unit tests with pytest
- [ ] CI/CD pipeline (GitHub Actions)
- [ ] AWS Glue deployment
- [ ] Read s3 files directly and process
- [ ] Real-time processing with Kafka

---

## ğŸ™ Acknowledgments

Built following the Data Engineering tutorial by **[Manish Kumar](https://www.youtube.com/playlist?list=PLTsNSGeIpGnHdXyLOeZ4m6tIRPvV_2jZd)**

---


